{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:300%;\">AI4EduRes'2023: <br />Fine-Tuning RoBERTa for Downstream Tasks</h1>\n",
    "<p>This notebook details how to use RoBERTa with HuggingFace.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📽️ IMDb Review Sentiment Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Platform Check\n",
    "Ensure we're on an ARM environment. \n",
    "\n",
    "NOTE:  If you are not on an ARM environment, update `params.device` to `torch.device('cuda' if torch.cuda.is_available() else 'cpu')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We're Armed: macOS-13.0-arm64-i386-64bit\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "\n",
    "if platform.platform() == 'macOS-13.0-arm64-i386-64bit':\n",
    "    print(f\"We're Armed: {platform.platform()}\")\n",
    "else:\n",
    "    print(f\"WARNING! NOT ARMED: {platform.platform()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Settings\n",
    "We begin by importing the necessary packages. Two imports to note are params and utils:\n",
    "<ul>\n",
    "\n",
    "**`params.py`** : Contains parameters shared between all three notebooks.\n",
    "\n",
    "**`utils.py`** : Contains helper functions for visualizations in this notebook.\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import params\n",
    "from utils import *\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "from transformers import RobertaForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "# suppress model warning\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we adjust any parameters that may be unique to this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust parameters if necessary\n",
    "params.num_labels = 2\n",
    "params.output_dir = \"models/imdb_hf\"\n",
    "params.max_length = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDb\n",
    "\n",
    "For this notebook, I've prepared a train/validate/test split of the IMDb movie review dataset. I've also mapped the string sentiment labels to binary integers and renamed the columns to \"text\" and \"label\". I've saved that dataset as a HuggingFace dataset object for the purposes of demonstrating the HuggingFace framework. Here, I load the dataset from a local directory. \n",
    "\n",
    "HuggingFace also offers a large selection of datasets that can be loaded from their remote repository using `from datasets import load_dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration jahjinx--IMDb_movie_reviews-d7ed51e8fa5a21e7\n",
      "Found cached dataset csv (/Users/jarradjinx/.cache/huggingface/datasets/jahjinx___csv/jahjinx--IMDb_movie_reviews-d7ed51e8fa5a21e7/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f35bbe99803c4e9c80f34397c6ad25f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imdb = load_dataset(\"jahjinx/IMDb_movie_reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 36000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 4000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Beautifully photographed and ably acted, generally, but the writing is very slipshod. There are scenes of such unbelievability that there is no joy in the watching. The fact that the young lover has a twin brother, for instance, is so contrived that I groaned out loud. And the \"emotion-light bulb connection\" seems gimmicky, too.<br /><br />I don\\'t know, though. If you have a few glasses of wine and feel like relaxing with something pretty to look at with a few flaccid comedic scenes, this is a pretty good movie. No major effort on the part of the viewer required. But Italian film, especially Italian comedy, is usually much, much better than this.',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View Example\n",
    "imdb[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess\n",
    "Below, we prepare our input text sequences to be accepted by the model. This involves tokenization and encoding of our sequences.\n",
    "\n",
    "<ul>\n",
    "\n",
    "<b>Tokenization</b> :  Splitting strings into word or sub-word token strings <br />\n",
    "<b>Encoding</b> : Converting those token strings into integers<br />\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Tokenization & Encoding\n",
    "\n",
    "Before tokenizing and encoding our training data, we briefly explore RoBERTa's tokenization process. RoBERTa uses a type of Byte-Pair Encoding that not only creates subword units using bytes rather than unicode characters, but also allows it to learn a modestly-sized subword vocabulary without introducing any “unknown” tokens. You can learn more about Byte-Pair Encoding here: https://huggingface.co/docs/transformers/tokenizer_summary. <br />\n",
    "\n",
    "\n",
    "Most text preprocessing, beyond tokenization and encoding, is largely unnecessary for RoBERTa-based models. For example, the removal of stop words, a common text preprocessing technique, is unnecessary as RoBERTa's dictionary not only includes these words, but the training corpus and pre-training process include these words as well. Casing is also accounted for, meaning RoBERTa will process and infer from casing in input text.<br />\n",
    "\n",
    "We can see examples of sub-word tokenization, distinction between vocabulary words of different casing, special tokens, and how characters such as spaces are represented below.<br />\n",
    "\n",
    "Our first example shows how the RoBERTa encodes the same word with different casing. Our second example illustrates how it handles non-words, spaces, and attention masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒══════════╤═════════════╤══════════════════╕\n",
      "│ Tokens   │   Token IDs │   Attention Mask │\n",
      "╞══════════╪═════════════╪══════════════════╡\n",
      "│ <s>      │           0 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ Hello    │       31414 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ </s>     │           2 │                1 │\n",
      "╘══════════╧═════════════╧══════════════════╛\n",
      "╒══════════╤═════════════╤══════════════════╕\n",
      "│ Tokens   │   Token IDs │   Attention Mask │\n",
      "╞══════════╪═════════════╪══════════════════╡\n",
      "│ <s>      │           0 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ hello    │       42891 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ </s>     │           2 │                1 │\n",
      "╘══════════╧═════════════╧══════════════════╛\n",
      "╒══════════╤═════════════╤══════════════════╕\n",
      "│ Tokens   │   Token IDs │   Attention Mask │\n",
      "╞══════════╪═════════════╪══════════════════╡\n",
      "│ <s>      │           0 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ he       │         700 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ L        │         574 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ lo       │        4082 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ </s>     │           2 │                1 │\n",
      "╘══════════╧═════════════╧══════════════════╛\n"
     ]
    }
   ],
   "source": [
    "# View Encoding of Title-Case \"Hello\"\n",
    "sequence_1 = \"Hello\"\n",
    "encoding_1 = params.tokenizer.encode_plus(\n",
    "                        sequence_1,\n",
    "                        add_special_tokens = True,\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt'\n",
    "                   )\n",
    "\n",
    "\n",
    "token_id_1 = encoding_1['input_ids']\n",
    "attention_masks_1 = encoding_1['attention_mask']\n",
    "\n",
    "# View Encoding of Lower-Case \"hello\"\n",
    "sequence_2 = \"hello\"\n",
    "encoding_2 = params.tokenizer.encode_plus(\n",
    "                        sequence_2,\n",
    "                        add_special_tokens = True,\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt'\n",
    "                   )\n",
    "\n",
    "\n",
    "token_id_2 = encoding_2['input_ids']\n",
    "attention_masks_2 = encoding_2['attention_mask']\n",
    "\n",
    "# View Encoding of Lower-Case \"hello\"\n",
    "sequence_3 = \"heLlo\"\n",
    "encoding_3 = params.tokenizer.encode_plus(\n",
    "                        sequence_3,\n",
    "                        add_special_tokens = True,\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt'\n",
    "                   )\n",
    "\n",
    "\n",
    "token_id_3 = encoding_3['input_ids']\n",
    "attention_masks_3 = encoding_3['attention_mask']\n",
    "\n",
    "print_sentence_encoding(token_id_1, attention_masks_1)\n",
    "print_sentence_encoding(token_id_2, attention_masks_2)\n",
    "print_sentence_encoding(token_id_3, attention_masks_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒══════════╤═════════════╤══════════════════╕\n",
      "│ Tokens   │   Token IDs │   Attention Mask │\n",
      "╞══════════╪═════════════╪══════════════════╡\n",
      "│ <s>      │           0 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ Please   │        6715 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ Ġreorgan │       22208 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ ize      │        2072 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ Ġthese   │         209 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ Ġletters │        5430 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ :        │          35 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ Ġ        │        1437 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ Ġ        │        1437 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ Ġthe     │           5 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ Ġs       │         579 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ ld       │        4779 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ kj       │       36085 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ Ġu       │        1717 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ g        │         571 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ </s>     │           2 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ <pad>    │           1 │                0 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ <pad>    │           1 │                0 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ <pad>    │           1 │                0 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ <pad>    │           1 │                0 │\n",
      "╘══════════╧═════════════╧══════════════════╛\n"
     ]
    }
   ],
   "source": [
    "sequence = \"Please reorganize these letters:   the sldkj ug\"\n",
    "test = params.tokenizer.encode_plus(\n",
    "                        sequence,\n",
    "                        add_special_tokens = True,\n",
    "                        max_length = 20,\n",
    "                        padding='max_length',\n",
    "                        truncation=True,\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt'\n",
    "                   )\n",
    "\n",
    "\n",
    "token_id = test['input_ids']\n",
    "attention_masks =test['attention_mask']\n",
    "\n",
    "print_sentence_encoding(token_id, attention_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing Training Data\n",
    "\n",
    "The preprocessing function below will tokenize and encode our strings according to RoBERTa's pre-defined tokenization and encoding dictionary. When used in conjunction with HuggingFace Datasets and the mapping function, the result is the addition of token/input_ids and attention mask columns to our Dataset:\n",
    "\n",
    "<ul>\n",
    "\n",
    "<b>token/input ids</b> : A list of integers that represent our tokenized and encoded string<br />\n",
    "<b>attention masks</b> : A list of 1's and 0's mapped to each token id. 1 represents an id to which the model should apply attention and 0 represents an id to which the model may not apply attention (padding tokens, for example).\n",
    "\n",
    "</ul>\n",
    "\n",
    "It is important to note that our preprocessing function's tokenizer is set only to truncate sequences longer than `max_length` to `max_length`. Sequences shorter than `max_length` will need to be padded,  which is possible via the tokenizer. However, padding at this point will pad all sequences to `max_length` which is unnecessary and computationally inefficient as explained below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return params.tokenizer(examples[\"text\"], \n",
    "                            max_length = params.max_length, # 256 in this example\n",
    "                            truncation=True) # truncate sequences longer than max_length to max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_imdb = imdb.map(preprocess_function, batched=True)\n",
    "\n",
    "# removing unneeded columns avoids warning in training loop\n",
    "tokenized_imdb = tokenized_imdb.remove_columns(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 36000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 4000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 152\n",
      "[0, 29287, 34717, 16372, 8, 4091, 352, 8337, 6, 3489, 6, 53, 5, 2410, 16, 182, 9215, 1193, 1630, 4, 345, 32, 5422, 9, 215, 49856, 4484, 14, 89, 16, 117, 5823, 11, 5, 2494, 4, 20, 754, 14, 5, 664, 16095, 34, 10, 9544, 2138, 6, 13, 4327, 6, 16, 98, 8541, 36040, 14, 38, 11491, 22597, 66, 7337, 4, 178, 5, 22, 991, 19187, 12, 6991, 32384, 2748, 113, 1302, 40585, 14963, 6, 350, 49069, 3809, 1589, 49007, 3809, 48709, 100, 218, 75, 216, 6, 600, 4, 318, 47, 33, 10, 367, 11121, 9, 3984, 8, 619, 101, 19448, 19, 402, 1256, 7, 356, 23, 19, 10, 367, 2342, 7904, 808, 29045, 5422, 6, 42, 16, 10, 1256, 205, 1569, 4, 440, 538, 1351, 15, 5, 233, 9, 5, 18754, 1552, 4, 125, 3108, 822, 6, 941, 3108, 5313, 6, 16, 2333, 203, 6, 203, 357, 87, 42, 4, 2]\n",
      "<s>Beautifully photographed and ably acted, generally, but the writing is very slipshod. There are scenes of such unbelievability that there is no joy in the watching. The fact that the young lover has a twin brother, for instance, is so contrived that I groaned out loud. And the \"emotion-light bulb connection\" seems gimmicky, too.<br /><br />I don't know, though. If you have a few glasses of wine and feel like relaxing with something pretty to look at with a few flaccid comedic scenes, this is a pretty good movie. No major effort on the part of the viewer required. But Italian film, especially Italian comedy, is usually much, much better than this.</s>\n"
     ]
    }
   ],
   "source": [
    "# view and decode a single sequence\n",
    "print(\"Length:\", len(tokenized_imdb['train'][0]['input_ids']))\n",
    "print(tokenized_imdb['train'][0]['input_ids'])\n",
    "print(params.tokenizer.decode(tokenized_imdb['train'][0]['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Data Collator\n",
    "\n",
    "By creating a collator that collects and combines portions of our dataset, we are able to refine the way data is handed to the model. Specifically, the DataCollatorWithPadding will collect our data into batches of a pre-defined size (`params.batch_size`). It will then pad all sequences within that batch to the length of the longest sequence within given batch. Finally, it converts the batch to a tensor before handing it to the model. This process is called dynamic padding.\n",
    "\n",
    "Dynamic padding increases the efficiency of forwarding data through the model as sequences may be shorter than `max_length`. If we pad our sequences and convert them to tensors without collation, every sequence given to the model will be `max_length`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center;\"><img src=\"presentation_resources/collate.png\" width=\"750\" align=\"center\"><figcaption style=\"text-align: center;\">Bednarski , M (2022). Understand collate_fn in PyTorch [Blog post]. Retrieved from https://plainenglish.io/blog/understanding-collate-fn-in-pytorch-f9d1742647d3</figcaption></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=params.tokenizer,\n",
    "                                        padding='max_length',\n",
    "                                        max_length=params.max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training Metrics\n",
    "\n",
    "If you would like to view metrics other than loss during training, specifically in relation to the validation loop, you may load those metrics in a computational function as shown below. This function must pass predictions and labels to [compute](https://huggingface.co/docs/evaluate/main/en/package_reference/main_classes#evaluate.EvaluationModule.compute) to calculate your defined metrics. \n",
    "\n",
    "This function will be given to the Trainer below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    metric1 = evaluate.load(\"accuracy\")\n",
    "    metric2 = evaluate.load(\"f1\")\n",
    "    \n",
    "    # get prediction logits and true labels\n",
    "    logits, labels = eval_pred\n",
    "    \n",
    "    # get predictions from logits\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    # pass predictions and true labels to metric functions\n",
    "    accuracy = metric1.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    f1 = metric2.compute(predictions=predictions, references=labels)[\"f1\"]\n",
    "    \n",
    "    return {\"accuracy\": accuracy, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & View Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==============================================================================================================\n",
       "Layer (type:depth-idx)                                       Output Shape              Param #\n",
       "==============================================================================================================\n",
       "RobertaForSequenceClassification                             [1, 2]                    --\n",
       "├─RobertaModel: 1-1                                          [1, 512, 768]             --\n",
       "│    └─RobertaEmbeddings: 2-1                                [1, 512, 768]             --\n",
       "│    │    └─Embedding: 3-1                                   [1, 512, 768]             38,603,520\n",
       "│    │    └─Embedding: 3-2                                   [1, 512, 768]             768\n",
       "│    │    └─Embedding: 3-3                                   [1, 512, 768]             394,752\n",
       "│    │    └─LayerNorm: 3-4                                   [1, 512, 768]             1,536\n",
       "│    │    └─Dropout: 3-5                                     [1, 512, 768]             --\n",
       "│    └─RobertaEncoder: 2-2                                   [1, 512, 768]             --\n",
       "│    │    └─ModuleList: 3-6                                  --                        85,054,464\n",
       "├─RobertaClassificationHead: 1-2                             [1, 2]                    --\n",
       "│    └─Dropout: 2-3                                          [1, 768]                  --\n",
       "│    └─Linear: 2-4                                           [1, 768]                  590,592\n",
       "│    └─Dropout: 2-5                                          [1, 768]                  --\n",
       "│    └─Linear: 2-6                                           [1, 2]                    1,538\n",
       "==============================================================================================================\n",
       "Total params: 124,647,170\n",
       "Trainable params: 124,647,170\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 124.65\n",
       "==============================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 427.83\n",
       "Params size (MB): 498.59\n",
       "Estimated Total Size (MB): 926.42\n",
       "=============================================================================================================="
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the RobertaForSequenceClassification model\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base',\n",
    "                                                         num_labels = params.num_labels,\n",
    "                                                         output_attentions = False,\n",
    "                                                         output_hidden_states = False,\n",
    "                                                         )\n",
    "\n",
    "# view the model summary by passing dummy data of compatible shape\n",
    "from torchinfo import summary\n",
    "summary(model, input_size=(1, 512), dtypes=['torch.IntTensor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "Before training, we define all of our training hyperparameters via [TrainingArguments](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments). The only required parameter is `output_dir` while all other arguments have default values set. Here, we define many of our own parameters.\n",
    "\n",
    "We then instantiate the [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) with our model, training arguments, dataset splits, tokenizer, collator and metrics function.\n",
    "\n",
    "Finally, we start fine-tuning our model by calling [train()](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.train)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    disable_tqdm=False, # show training progress\n",
    "    output_dir=params.output_dir, # \"imdb_hf\"\n",
    "    learning_rate=params.learning_rate,\n",
    "    optim=\"adamw_torch\",\n",
    "    per_device_train_batch_size=params.batch_size,\n",
    "    per_device_eval_batch_size=params.batch_size,\n",
    "    num_train_epochs=params.epochs,\n",
    "    evaluation_strategy=\"epoch\", # eval at each epoch\n",
    "    logging_strategy=\"epoch\", # show info each epoch  \n",
    "    save_strategy=\"epoch\", # save at each epoch\n",
    "    load_best_model_at_end=True,\n",
    "    use_mps_device=True, # use MPS, remove if using GPU\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_imdb[\"train\"],\n",
    "    eval_dataset=tokenized_imdb[\"validation\"],\n",
    "    tokenizer=params.tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model to our training data - fine-tuning\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: left;\"><img src=\"presentation_resources/hf_training.png\"  align=\"left\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Once a model is fine-tuned, we can load the model, its tokenizer, pre-process new input and generate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports for Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import RobertaTokenizer\n",
    "from transformers import TextClassificationPipeline\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infer Manually w/HuggingFace\n",
    "First, load our selected fine-tuned model and its tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file models/imdb_hf/checkpoint-4500/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"models/imdb_hf/checkpoint-4500\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file models/imdb_hf/checkpoint-4500/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at models/imdb_hf/checkpoint-4500.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "PATH = 'models/imdb_hf/checkpoint-4500'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(PATH, local_files_only=True)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(PATH, local_files_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we tokenize our test input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   0,  100, 4157,   42, 1569,  328,    2],\n",
      "        [   0,  100,  657,   42, 1569,  328,    2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# input = tokenizer(\"I hate this movie!\", return_tensors=\"pt\")\n",
    "inputs = tokenizer([\"I hate this movie!\", \"I love this movie!\"], return_tensors=\"pt\")\n",
    "\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward those inputs through the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.3273, -3.2752],\n",
       "        [-3.2370,  3.5497]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classes are mapped by index. Because our classification head uses a softmax function on the output layer, the index with the highest value corresponds to our predicted label. In this case, 0 = \"Negative (Sentiment)\" and 1 = \"Positive (Sentiment)\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>I hate this movie!</s> 0\n",
      "<s>I love this movie!</s> 1\n"
     ]
    }
   ],
   "source": [
    "# loop through logits\n",
    "for i, v in enumerate(logits):\n",
    "    # get index of largest value\n",
    "    predicted_class_id = v.argmax().item()\n",
    "    # get & decode input, match with predicted_class_id\n",
    "    print(params.tokenizer.decode((inputs['input_ids'][i])), predicted_class_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infer with HugginFace Pipeline\n",
    "\n",
    "HuggingFace's pipeline method allows us to streamline the inference process further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file models/imdb_hf/checkpoint-4500/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"models/imdb_hf/checkpoint-4500\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file models/imdb_hf/checkpoint-4500/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at models/imdb_hf/checkpoint-4500.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "PATH = 'models/imdb_hf/checkpoint-4500'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(PATH, local_files_only=True)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(PATH, local_files_only=True)\n",
    "\n",
    "# define pipeline\n",
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, top_k=2, max_length=512, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'LABEL_0', 'score': 0.9987990856170654},\n",
       "  {'label': 'LABEL_1', 'score': 0.001200946164317429}]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"i hate this movie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Whole Test Set & Evaluate\n",
    "We will use the HuggingFace pipeline with a loop in order to generate predictions for our entire test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Test Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Test Data\n",
    "imdb = load_from_disk(\"data/inter_IMDB_sentiment/IMDb.hf\")\n",
    "\n",
    "imdb_test = imdb['test']\n",
    "\n",
    "imdb_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test 10000: 100%|██████████| 10000/10000 [21:30<00:00,  7.75test/s]\n"
     ]
    }
   ],
   "source": [
    "# get sequences\n",
    "test_input = imdb_test['text']\n",
    "\n",
    "test_output = []\n",
    "\n",
    "# pipe sequences to tokenizer -> model\n",
    "with tqdm(test_input, unit=\"test\") as prog:\n",
    "    for step, test in enumerate(prog):\n",
    "        prog.set_description(f\"Test {step+1}\")\n",
    "        # append results to test_output list\n",
    "        test_output.append(pipe(test)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Output Slice:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'label': 'LABEL_0', 'score': 0.9918249845504761},\n",
       "  {'label': 'LABEL_1', 'score': 0.008175036869943142}],\n",
       " [{'label': 'LABEL_0', 'score': 0.9990043044090271},\n",
       "  {'label': 'LABEL_1', 'score': 0.0009957626461982727}],\n",
       " [{'label': 'LABEL_0', 'score': 0.9987474679946899},\n",
       "  {'label': 'LABEL_1', 'score': 0.0012525408528745174}],\n",
       " [{'label': 'LABEL_0', 'score': 0.9980292916297913},\n",
       "  {'label': 'LABEL_1', 'score': 0.001970750279724598}],\n",
       " [{'label': 'LABEL_1', 'score': 0.9984378218650818},\n",
       "  {'label': 'LABEL_0', 'score': 0.0015621976926922798}]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Test Output Slice:\")\n",
    "test_output[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "['LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_1', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0']\n"
     ]
    }
   ],
   "source": [
    "# parse target predictions to new list\n",
    "predictions = []\n",
    "\n",
    "for i in test_output:\n",
    "    predictions.append(i[0]['label'])\n",
    "    \n",
    "print(len(predictions))\n",
    "print(predictions[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# remove \"LABEL_\" and cast as int\n",
    "for i, v in enumerate(predictions):\n",
    "    predictions[i] = int(v.replace(\"LABEL_\",\"\"))\n",
    "\n",
    "print(predictions[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9506\n",
      "F1:  0.951011503371678\n"
     ]
    }
   ],
   "source": [
    "# get accuracy and F1\n",
    "acc = accuracy_score(imdb_test['label'], predictions)\n",
    "f1 = f1_score(imdb_test['label'], predictions)\n",
    "\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"F1: \", f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('workshop_present_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "67962405cad10eeeccd3e4011c192b84211e6b516afcf058c81650d23e67a1ed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
