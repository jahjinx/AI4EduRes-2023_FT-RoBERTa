{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:300%;\">AI4EduRes'2023: <br />Fine-Tuning RoBERTa for Downstream Tasks</h1>\n",
    "<p>This notebook details how to use RoBERTa with Pytorch and minimal use of HuggingFace features. Additionally, this notebook illustrates RoBERTa with dynamic padding of inputs.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“½ï¸ IMDb Review Sentiment Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Platform Check\n",
    "Ensure we're on an ARM environment. \n",
    "\n",
    "NOTE:  If you are not on an ARM environment, update `params.device` to `torch.device('cuda' if torch.cuda.is_available() else 'cpu')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We're Armed: macOS-13.0-arm64-i386-64bit\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "\n",
    "if platform.platform() == 'macOS-13.0-arm64-i386-64bit':\n",
    "    print(f\"We're Armed: {platform.platform()}\")\n",
    "else:\n",
    "    print(f\"WARNING! NOT ARMED: {platform.platform()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import params\n",
    "\n",
    "\"\"\"\n",
    "utils imports the following functions:\n",
    "    preprocessing, preprocessing_trunkless, preprocessing_dyna\n",
    "    collate, print_sentence_encoding\n",
    "\"\"\"\n",
    "from utils import *\n",
    "from trainer import *\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import RobertaForSequenceClassification\n",
    "\n",
    "# suppress model warning\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# set logging level\n",
    "import logging\n",
    "logging.basicConfig(format='%(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.num_labels = 2\n",
    "params.output_dir = \"imdb_256_dyna\"\n",
    "params.max_length = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDb\n",
    "\n",
    "For this notebook, I've prepared a train/validate/test split of the IMDb movie review dataset. I've also mapped the string sentiment labels to binary integers and renamed the columns to \"text\" and \"label\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beautifully photographed and ably acted, gener...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Well, where to start describing this celluloid...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I first caught the movie on its first run on H...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I love Umberto Lenzi's cop movies -- ROME ARME...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I generally won't review movies I haven't seen...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Beautifully photographed and ably acted, gener...      0\n",
       "1  Well, where to start describing this celluloid...      0\n",
       "2  I first caught the movie on its first run on H...      1\n",
       "3  I love Umberto Lenzi's cop movies -- ROME ARME...      0\n",
       "4  I generally won't review movies I haven't seen...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_path = 'data/inter_IMDB_sentiment/IMDB_train.csv'\n",
    "validate_dataset_path = 'data/inter_IMDB_sentiment/IMDB_validate.csv'\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(train_dataset_path)\n",
    "validate_df = pd.read_csv(validate_dataset_path)\n",
    "\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 36000 entries, 0 to 35999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    36000 non-null  object\n",
      " 1   label   36000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 562.6+ KB\n",
      "None\n",
      "\n",
      "train_df Value Counts\n",
      "1    18056\n",
      "0    17944\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# view training dataset\n",
    "print(\"train_df Info:\")\n",
    "print(train_df.info())\n",
    "print(\"\\ntrain_df Value Counts\")\n",
    "print(train_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validate_df Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4000 entries, 0 to 3999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    4000 non-null   object\n",
      " 1   label   4000 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 62.6+ KB\n",
      "None\n",
      "\n",
      " validate_df Value Counts\n",
      "0    2012\n",
      "1    1988\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# view validation dataset\n",
    "print(\"validate_df Info:\")\n",
    "print(validate_df.info())\n",
    "print(\"\\n validate_df Value Counts\")\n",
    "print(validate_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess - w/Dynamic Padding\n",
    "Below, we prepare our input text sequences to be accepted by the model. This involves tokenization and encoding of our sequences.\n",
    "\n",
    "<ul>\n",
    "\n",
    "<b>Tokenization</b> :  Splitting strings into word or sub-word token strings <br />\n",
    "<b>Encoding</b> : Converting those token strings into integers<br />\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Tokenization & Encoding\n",
    "\n",
    "Below, we briefly explore RoBERTa's tokenization process. RoBERTa uses a type of Byte-Pair Encoding that creates subword units using bytes rather than unicode characters. This allows it to learn a modestly-sized subword vocabulary without introducing any â€œunknownâ€ tokens. You can learn more about Byte-Pair Encoding here: https://huggingface.co/docs/transformers/tokenizer_summary. <br />\n",
    "\n",
    "\n",
    "Most text preprocessing, beyond tokenization and encoding, is largely unnecessary for RoBERTa-based models. For example, the removal of stop words, a common text preprocessing technique, is unnecessary as RoBERTa's dictionary not only includes these words, but the training corpus and pre-training process include these words as well. Casing is also accounted for, meaning RoBERTa will process and infer from casing in input text.<br />\n",
    "\n",
    "We can see examples of sub-word tokenization, distinction between vocabulary words of different casing, special tokens, and how characters such as spaces are represented below.<br />\n",
    "\n",
    "Our first example shows how the RoBERTa encodes the same word with different casing. Our second example illustrates how it handles non-words, spaces, and attention masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â•’â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â••\n",
      "â”‚ Tokens   â”‚   Token IDs â”‚   Attention Mask â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ <s>      â”‚           0 â”‚                1 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ Hello    â”‚       31414 â”‚                1 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ </s>     â”‚           2 â”‚                1 â”‚\n",
      "â•˜â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•›\n",
      "â•’â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â••\n",
      "â”‚ Tokens   â”‚   Token IDs â”‚   Attention Mask â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ <s>      â”‚           0 â”‚                1 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ hello    â”‚       42891 â”‚                1 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ </s>     â”‚           2 â”‚                1 â”‚\n",
      "â•˜â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•›\n"
     ]
    }
   ],
   "source": [
    "# View Encoding of Title-Case \"Hello\"\n",
    "sequence_1 = \"Hello\"\n",
    "encoding_1 = params.tokenizer.encode_plus(\n",
    "                        sequence_1,\n",
    "                        add_special_tokens = True,\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt'\n",
    "                   )\n",
    "\n",
    "\n",
    "token_id_1 = encoding_1['input_ids']\n",
    "attention_masks_1 = encoding_1['attention_mask']\n",
    "\n",
    "# View Encoding of Lower-Case \"hello\"\n",
    "sequence_2 = \"hello\"\n",
    "encoding_2 = params.tokenizer.encode_plus(\n",
    "                        sequence_2,\n",
    "                        add_special_tokens = True,\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt'\n",
    "                   )\n",
    "\n",
    "\n",
    "token_id_2 = encoding_2['input_ids']\n",
    "attention_masks_2 = encoding_2['attention_mask']\n",
    "\n",
    "\n",
    "print_sentence_encoding(token_id_1, attention_masks_1)\n",
    "print_sentence_encoding(token_id_2, attention_masks_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â•’â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â••\n",
      "â”‚ Tokens   â”‚   Token IDs â”‚   Attention Mask â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ <s>      â”‚           0 â”‚                1 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ Hello    â”‚       31414 â”‚                1 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ Ä world   â”‚         232 â”‚                1 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ ,        â”‚           6 â”‚                1 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ Ä         â”‚        1437 â”‚                1 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ Ä         â”‚        1437 â”‚                1 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ Ä the     â”‚           5 â”‚                1 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ Ä s       â”‚         579 â”‚                1 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ ld       â”‚        4779 â”‚                1 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ kj       â”‚       36085 â”‚                1 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ Ä u       â”‚        1717 â”‚                1 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ g        â”‚         571 â”‚                1 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ </s>     â”‚           2 â”‚                1 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ <pad>    â”‚           1 â”‚                0 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ <pad>    â”‚           1 â”‚                0 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ <pad>    â”‚           1 â”‚                0 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ <pad>    â”‚           1 â”‚                0 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ <pad>    â”‚           1 â”‚                0 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ <pad>    â”‚           1 â”‚                0 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ <pad>    â”‚           1 â”‚                0 â”‚\n",
      "â•˜â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•›\n"
     ]
    }
   ],
   "source": [
    "sequence = \"Hello world,   the sldkj ug\"\n",
    "test = params.tokenizer.encode_plus(\n",
    "                        sequence,\n",
    "                        add_special_tokens = True,\n",
    "                        max_length = 20,\n",
    "                        padding='max_length',\n",
    "                        truncation=True,\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt'\n",
    "                   )\n",
    "\n",
    "\n",
    "token_id = test['input_ids']\n",
    "attention_masks =test['attention_mask']\n",
    "\n",
    "print_sentence_encoding(token_id, attention_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing Training Data\n",
    "\n",
    "The preprocessing function, located in `utils.py`, will tokenize and encode our strings according to RoBERTa's pre-defined tokenization and encoding dictionary. The returned result is a dictionary that includes:\n",
    "\n",
    "<b>token/input ids</b> : A list of integers that represent our tokenized and encoded string<br />\n",
    "<b>attention masks</b> : A list of 1's and 0's mapped to each token id. 1 represents an id to which the model should apply attention and 0 represents an id to which the model may not apply attention (padding tokens, for example).\n",
    "\n",
    "The dictionary is parsed out to lists which we can then process further into a format that is acceptable to our collation function later in this notebook. We also note the following:\n",
    "\n",
    "- Our preprocessing function includes Huggingface's `tokenizer.encode_plus` method, for simplicity.\n",
    "- The following can be done more succinctly, for example, with our data stored as a `datasets.Dataset` object which enables features such as mapping functions to all splits of our data at once. However, the below loop is a bit more illustrative.\n",
    "- We are not yet working with tensors at this point.\n",
    "  - Tensors require nested tensors on the same dimension to be of the same length, therefore we construct a list of lists at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_token_id = []\n",
    "train_attention_masks = []\n",
    "\n",
    "# encode training text\n",
    "for sample in train_df.text.values:\n",
    "  encoding_dict = preprocessing_dyna(sample, params.tokenizer)\n",
    "  # parse encoding dict to lists\n",
    "  train_token_id.append(encoding_dict['input_ids']) \n",
    "  train_attention_masks.append(encoding_dict['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_token_id = []\n",
    "validate_attention_masks = []\n",
    "\n",
    "# encode validation text \n",
    "for sample in validate_df.text.values:\n",
    "  encoding_dict = preprocessing_dyna(sample, params.tokenizer)  \n",
    "  # parse encoding dict to lists\n",
    "  validate_token_id.append(encoding_dict['input_ids']) \n",
    "  validate_attention_masks.append(encoding_dict['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we show that our inputs are:\n",
    "1. Truncated to `max_length`, 256 tokens, if longer than `max_length`\n",
    "2. Not yet padded to `max_length`, if shorter than `max_length`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: [num_samples]\n",
      "36000 \n",
      "\n",
      "Length: 161\n",
      "[0, 863, 11278, 8, 11776, 7815, 32, 233, 9, 41, 8180, 15893, 9, 559, 822, 6644, 4, 20, 9004, 1956, 197, 5478, 14, 51, 860, 7, 20489, 5, 3157, 77, 5, 168, 8, 433, 7313, 7018, 8807, 8, 1719, 62, 5, 1061, 3817, 5, 1281, 529, 11, 8432, 2457, 3321, 4, 85, 16, 3668, 17052, 7, 192, 141, 209, 7850, 1159, 33, 57, 9441, 8, 385, 31756, 30, 5, 9004, 249, 8, 3828, 7, 1789, 11, 559, 7341, 13, 3981, 2788, 3731, 8, 25, 13543, 13, 643, 2163, 4, 20, 129, 9327, 631, 59, 42, 1569, 16, 14, 24, 40, 45, 1338, 5, 4007, 15444, 11, 6171, 25, 24, 40, 129, 28, 2343, 24, 11327, 8, 45, 28, 703, 15, 569, 50, 10843, 15, 2384, 49069, 3809, 1589, 49007, 3809, 48709, 133, 559, 822, 16, 505, 25, 24, 64, 836, 92, 17403, 8, 8339, 88, 2632, 743, 8, 34, 10, 774, 7, 310, 25, 41, 23356, 9, 5, 15444, 4, 2]\n",
      "<s>Jarl and Moodysson are part of an dying breed of political film makers. The Swedish population should appreciate that they try to uncover the truth when the government and media actively distorts and cover up the events surrounding the EU meeting in Gothenburg. It is absolutely heartbreaking to see how these innocent kids have been abused and drugged by the Swedish police and convicted to prison in political trials for sending text messages and as revenge for others actions. The only unfortunate thing about this movie is that it will not reach the broad masses in Sweden as it will only be shown it theaters and not be released on video or aired on television.<br /><br />The political film is important as it can bring new perspectives and insight into complex issues and has a role to play as an educator of the masses.</s>\n"
     ]
    }
   ],
   "source": [
    "# view token_id shape\n",
    "print(\"Size: [num_samples]\")\n",
    "print(len(train_token_id), \"\\n\")\n",
    "\n",
    "# view and decode a single sequence\n",
    "print(\"Length:\", len(train_token_id[(13)]))\n",
    "print(train_token_id[(13)])\n",
    "print(params.tokenizer.decode(train_token_id[13]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we can see more clearly that our tokenized and encoded sequences are currently of different lengths.\n",
    "- Our first example has been truncated to the max token length. RoBERTa supports up to 512 but for efficiency we are using 256.\n",
    "- Our second example was short and did not need to be truncated. It is not padded to `max_length`.\n",
    "- With a fairly large number of 256+ token sequences, many of our batches will still include length 256 sequences and therefore the entire batch will be length 256 sequences. However, not all batches will be padded to `max_length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: [token_length]\n",
      "Token @ index 10:  256\n",
      "Token @ index 30,000:  15\n"
     ]
    }
   ],
   "source": [
    "# view token_id shape\n",
    "print(\"Size: [token_length]\")\n",
    "print(\"Token @ index 10: \", len(train_token_id[10]))\n",
    "print(\"Token @ index 30,000: \", len(train_token_id[30001]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format for Collation Function\n",
    "Within our collation function, located in `utils.py` , we use `tokenizer.pad` to pad each batch of input ids to the length of the longest sequence within said batch. `tokenizer.pad` expects data in the form of a list of dictionaries with the \"input_ids\" key present:<br /><br />\n",
    "[{\"input_ids\":Â [#,#,#], \"attention_mask:Â [#,#,#]},Â {\"input_ids\":Â [#,#,#], \"attention_mask:Â Â [#,#,#]}]\n",
    "\n",
    "These lists of dictionaries will be given to our dataloaders below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = []\n",
    "for i in range(len(train_token_id)):\n",
    "    train_features.append({'label': train_df.label.values[i], \n",
    "                           'input_ids': train_token_id[i], \n",
    "                           'attention_mask':train_attention_masks[i]})\n",
    "\n",
    "validate_features = []\n",
    "for i in range(len(validate_token_id)):\n",
    "    validate_features.append({'label': validate_df.label.values[i], \n",
    "                              'input_ids': validate_token_id[i],\n",
    "                              'attention_mask':validate_attention_masks[i]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we process some features through the collation function and view the output. This output contains our tokenized and encoded sequences, padded to the length of the longest sequence in the batch, as well as our attention masks and labels. This format is acceptable to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[    0, 29287, 34717,  ...,     1,     1,     1],\n",
       "         [    0,  8346,     6,  ..., 12137,     4,     2],\n",
       "         [    0,   100,    78,  ...,     1,     1,     1],\n",
       "         ...,\n",
       "         [    0,  4993,    38,  ...,   381, 19866,     2],\n",
       "         [    0,   100,  1017,  ...,    89,    16,     2],\n",
       "         [    0, 29375,   531,  ...,     4,   280,     2]]),\n",
       " tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]]),\n",
       " tensor([0, 0, 1, 0, 0, 1, 0, 1, 1, 0])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_test = collate(train_features[:10])\n",
    "col_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then give our lists of dictionaries to the dataloaders. `torch.utils.data.DataLoader` wraps an iterable around these lists to enable easy access to the samples.\n",
    "\n",
    "<a href=\"https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\"> View the Pytorch Datasets & Dataloaders Documentation</a>\n",
    "\n",
    "Before handing batches of data to the model, our dataloaders will first pass those batches to the collate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare DataLoader\n",
    "train_dataloader = DataLoader(\n",
    "            train_features,\n",
    "            batch_size = params.batch_size,\n",
    "            collate_fn=collate            \n",
    "        )\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            validate_features,\n",
    "            batch_size = params.batch_size,\n",
    "            collate_fn=collate\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Before training, we must instantiate our core model. Here, we download transformers.RobertaForSequenceClassification from HuggingFace, which is a RoBERTa model with a linear layer for sentence classification (or regression) on top of the pooled output.\n",
    "\n",
    "The model we load here may come from any source. Pytorch, for example, also makes pre-trained models available for fine-tuning. Depending on the source, we may or may not have to manually attach our own classification head. We use HuggingFace in this example as it is particularly simple to load a models with new classification heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==============================================================================================================\n",
       "Layer (type:depth-idx)                                       Output Shape              Param #\n",
       "==============================================================================================================\n",
       "RobertaForSequenceClassification                             [1, 2]                    --\n",
       "â”œâ”€RobertaModel: 1-1                                          [1, 512, 768]             --\n",
       "â”‚    â””â”€RobertaEmbeddings: 2-1                                [1, 512, 768]             --\n",
       "â”‚    â”‚    â””â”€Embedding: 3-1                                   [1, 512, 768]             38,603,520\n",
       "â”‚    â”‚    â””â”€Embedding: 3-2                                   [1, 512, 768]             768\n",
       "â”‚    â”‚    â””â”€Embedding: 3-3                                   [1, 512, 768]             394,752\n",
       "â”‚    â”‚    â””â”€LayerNorm: 3-4                                   [1, 512, 768]             1,536\n",
       "â”‚    â”‚    â””â”€Dropout: 3-5                                     [1, 512, 768]             --\n",
       "â”‚    â””â”€RobertaEncoder: 2-2                                   [1, 512, 768]             --\n",
       "â”‚    â”‚    â””â”€ModuleList: 3-6                                  --                        85,054,464\n",
       "â”œâ”€RobertaClassificationHead: 1-2                             [1, 2]                    --\n",
       "â”‚    â””â”€Dropout: 2-3                                          [1, 768]                  --\n",
       "â”‚    â””â”€Linear: 2-4                                           [1, 768]                  590,592\n",
       "â”‚    â””â”€Dropout: 2-5                                          [1, 768]                  --\n",
       "â”‚    â””â”€Linear: 2-6                                           [1, 2]                    1,538\n",
       "==============================================================================================================\n",
       "Total params: 124,647,170\n",
       "Trainable params: 124,647,170\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 124.65\n",
       "==============================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 427.83\n",
       "Params size (MB): 498.59\n",
       "Estimated Total Size (MB): 926.42\n",
       "=============================================================================================================="
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the RobertaForSequenceClassification model\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base',\n",
    "                                                         num_labels = params.num_labels,\n",
    "                                                         output_attentions = False,\n",
    "                                                         output_hidden_states = False,\n",
    "                                                         )\n",
    "\n",
    "# view the model summary by passing dummy data of compatible shape\n",
    "from torchinfo import summary\n",
    "summary(model, input_size=(1, 512), dtypes=['torch.IntTensor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Dataset: data/inter_IMDB_sentiment/IMDB_train.csv\n",
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# set model to device\n",
    "model.to(params.device)\n",
    "print(f\"Trained Dataset: {train_dataset_path}\")\n",
    "print(f\"Device: {params.device}\")\n",
    "\n",
    "# define optimizer\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=params.learning_rate) #roberta\n",
    "\n",
    "# initialize trainer\n",
    "trainer = Trainer(model=model,\n",
    "                  device=params.device,\n",
    "                  tokenizer=params.tokenizer,\n",
    "                  train_dataloader=train_dataloader,\n",
    "                  validation_dataloader=validation_dataloader,\n",
    "                  epochs=params.epochs,\n",
    "                  optimizer=optimizer,\n",
    "                  val_loss_fn=params.val_loss_fn,\n",
    "                  num_labels=params.num_labels,\n",
    "                  output_dir=params.output_dir,\n",
    "                  save_freq=params.save_freq,\n",
    "                  checkpoint_freq=params.checkpoint_freq,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model to our training data - fine-tuning\n",
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Once a model is fine-tuned, we can load the model, its tokenizer, pre-process new input, and generate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports for Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "from transformers import TextClassificationPipeline\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infer w/Pytorch\n",
    "\n",
    "Our first example quickly shows how to infer with Pytorch, which involves instantiating the pre-trained model and applying our fine-tuned model's state. We can then put the model into eval mode and run inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get State Dict Location & Tokenizer Path\n",
    "pt_check_path = 'models/imdb_512/E02_A0.96_F0.96/checkpoint.pt'\n",
    "tokenizer_path = 'models/imdb_512/E02_A0.96_F0.96'\n",
    "\n",
    "# Instantiate core model\n",
    "# This can be done with Huggingface, from local, from Pytorch, etc\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base',\n",
    "                                                         num_labels = params.num_labels,\n",
    "                                                         output_attentions = False,\n",
    "                                                         output_hidden_states = False,\n",
    "                                                         )\n",
    "\n",
    "# Get Tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(tokenizer_path, local_files_only=True)\n",
    "\n",
    "# load and apply fine-tuned model state to core model\n",
    "checkpoint = torch.load(pt_check_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# set to eval mode\n",
    "model.eval()\n",
    "\n",
    "# tokenize inputs\n",
    "inputs = tokenizer([\"I hate this movie!\", \"I love this movie!\"], return_tensors=\"pt\")\n",
    "\n",
    "# infer\n",
    "output = model(inputs['input_ids'], inputs['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get logits from output\n",
    "logits = output.logits\n",
    "\n",
    "# loop through logits\n",
    "for i, v in enumerate(logits):\n",
    "    # get index of largest value\n",
    "    predicted_class_id = v.argmax().item()\n",
    "    # get & decode input, match with predicted_class_id\n",
    "    print(params.tokenizer.decode((inputs['input_ids'][i])), predicted_class_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infer with HugginFace Pipeline\n",
    "\n",
    "HuggingFace's pipeline method allows us to streamline the inference process further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'models/imdb_512/E02_A0.96_F0.96'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(PATH, local_files_only=True)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(PATH, local_files_only=True)\n",
    "\n",
    "# define pipeline\n",
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, top_k=2, max_length=512, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe(\"i hate this movie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Whole Test Set & Evaluate\n",
    "We will use the HuggingFace pipeline with a loop in order to generate predictions for our entire test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Test Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Test Data\n",
    "test_df = pd.read_csv('data/inter_IMDB_sentiment/IMDB_test.csv')\n",
    "\n",
    "test_df = test_df.sample(n=500, random_state=1)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sequences\n",
    "test_input = test_df['text'].to_list()\n",
    "\n",
    "test_output = []\n",
    "\n",
    "# pipe sequences to tokenizer -> model\n",
    "with tqdm(test_input, unit=\"test\") as prog:\n",
    "    for step, test in enumerate(prog):\n",
    "        prog.set_description(f\"Test {step+1}\")\n",
    "        # append results to test_output list\n",
    "        test_output.append(pipe(test)[0])\n",
    "\n",
    "test_output[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse target predictions to new list\n",
    "predictions = []\n",
    "\n",
    "for i in test_output:\n",
    "    predictions.append(i[0]['label'])\n",
    "    \n",
    "print(len(predictions))\n",
    "print(predictions[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove \"LABEL_\" and cast as int\n",
    "for i, v in enumerate(predictions):\n",
    "    predictions[i] = int(v.replace(\"LABEL_\",\"\"))\n",
    "\n",
    "print(predictions[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get accuracy and F1\n",
    "acc = accuracy_score(test_df['label'], predictions)\n",
    "f1 = f1_score(test_df['label'], predictions)\n",
    "\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"F1: \", f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('itesd_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3c42b54925bdca82cdb5059acc0a21648e00763ff265e64872b54aa656b5d9d8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
