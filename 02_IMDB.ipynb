{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:300%;\">AI4EduRes'2023: <br />Fine-Tuning RoBERTa for Downstream Tasks</h1>\n",
    "<p>This notebook details how to use RoBERTa with Pytorch and minimal use of HuggingFace features. Additionally, this notebook illustrates RoBERTa without dynamic padding of inputs.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìΩÔ∏è IMDb Review Sentiment Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Platform Check\n",
    "Ensure we're on an ARM environment. \n",
    "\n",
    "NOTE:  If you are not on an ARM environment, update `params.device` to `torch.device('cuda' if torch.cuda.is_available() else 'cpu')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We're Armed: macOS-13.0-arm64-i386-64bit\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "\n",
    "if platform.platform() == 'macOS-13.0-arm64-i386-64bit':\n",
    "    print(f\"We're Armed: {platform.platform()}\")\n",
    "else:\n",
    "    print(f\"WARNING! NOT ARMED: {platform.platform()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import params\n",
    "\n",
    "\"\"\"\n",
    "utils imports the following functions:\n",
    "    preprocessing, preprocessing_trunkless, preprocessing_dyna\n",
    "    collate, print_sentence_encoding\n",
    "\"\"\"\n",
    "from utils import *\n",
    "from trainer import *\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import RobertaForSequenceClassification\n",
    "\n",
    "# suppress model warning\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# set logging level\n",
    "import logging\n",
    "logging.basicConfig(format='%(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust parameters if necessary\n",
    "params.num_labels = 2\n",
    "params.output_dir = \"imdb_512\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDb\n",
    "\n",
    "For this notebook, I've prepared a train/validate/test split of the IMDb movie review dataset. I've also mapped the string sentiment labels to binary integers and renamed the columns to \"text\" and \"label\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beautifully photographed and ably acted, gener...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Well, where to start describing this celluloid...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I first caught the movie on its first run on H...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I love Umberto Lenzi's cop movies -- ROME ARME...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I generally won't review movies I haven't seen...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Beautifully photographed and ably acted, gener...      0\n",
       "1  Well, where to start describing this celluloid...      0\n",
       "2  I first caught the movie on its first run on H...      1\n",
       "3  I love Umberto Lenzi's cop movies -- ROME ARME...      0\n",
       "4  I generally won't review movies I haven't seen...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_path = 'data/inter_IMDB_sentiment/IMDB_train.csv'\n",
    "validate_dataset_path = 'data/inter_IMDB_sentiment/IMDB_validate.csv'\n",
    "\n",
    "train_df = pd.read_csv(train_dataset_path)\n",
    "validate_df = pd.read_csv(validate_dataset_path)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 36000 entries, 0 to 35999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    36000 non-null  object\n",
      " 1   label   36000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 562.6+ KB\n",
      "None\n",
      "\n",
      "train_df Value Counts\n",
      "1    18056\n",
      "0    17944\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# view training dataset\n",
    "print(\"train_df Info:\")\n",
    "print(train_df.info())\n",
    "print(\"\\ntrain_df Value Counts\")\n",
    "print(train_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validate_df Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4000 entries, 0 to 3999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    4000 non-null   object\n",
      " 1   label   4000 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 62.6+ KB\n",
      "None\n",
      "\n",
      " validate_df Value Counts\n",
      "0    2012\n",
      "1    1988\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# view validation dataset\n",
    "print(\"validate_df Info:\")\n",
    "print(validate_df.info())\n",
    "print(\"\\n validate_df Value Counts\")\n",
    "print(validate_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess\n",
    "Below, we prepare our input text sequences to be accepted by the model. This involves tokenization and encoding of our sequences.\n",
    "\n",
    "<ul>\n",
    "\n",
    "<b>Tokenization</b> :  Splitting strings into word or sub-word token strings <br />\n",
    "<b>Encoding</b> : Converting those token strings into integers<br />\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Tokenization & Encoding\n",
    "\n",
    "Below, we briefly explore RoBERTa's tokenization process. RoBERTa uses a type of Byte-Pair Encoding that creates subword units using bytes rather than unicode characters. This allows it to learn a modestly-sized subword vocabulary without introducing any ‚Äúunknown‚Äù tokens. You can learn more about Byte-Pair Encoding here: https://huggingface.co/docs/transformers/tokenizer_summary. <br />\n",
    "\n",
    "\n",
    "Most text preprocessing, beyond tokenization and encoding, is largely unnecessary for RoBERTa-based models. For example, the removal of stop words, a common text preprocessing technique, is unnecessary as RoBERTa's dictionary not only includes these words, but the training corpus and pre-training process include these words as well. Casing is also accounted for, meaning RoBERTa will process and infer from casing in input text.<br />\n",
    "\n",
    "We can see examples of sub-word tokenization, distinction between vocabulary words of different casing, special tokens, and how characters such as spaces are represented below.<br />\n",
    "\n",
    "Our first example shows how the RoBERTa encodes the same word with different casing. Our second example illustrates how it handles non-words, spaces, and attention masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ïí‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï§‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï§‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïï\n",
      "‚îÇ Tokens   ‚îÇ   Token IDs ‚îÇ   Attention Mask ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ <s>      ‚îÇ           0 ‚îÇ                1 ‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ Hello    ‚îÇ       31414 ‚îÇ                1 ‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ </s>     ‚îÇ           2 ‚îÇ                1 ‚îÇ\n",
      "‚ïò‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïß‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïß‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïõ\n",
      "‚ïí‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï§‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï§‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïï\n",
      "‚îÇ Tokens   ‚îÇ   Token IDs ‚îÇ   Attention Mask ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ <s>      ‚îÇ           0 ‚îÇ                1 ‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ hello    ‚îÇ       42891 ‚îÇ                1 ‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ </s>     ‚îÇ           2 ‚îÇ                1 ‚îÇ\n",
      "‚ïò‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïß‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïß‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïõ\n"
     ]
    }
   ],
   "source": [
    "# View Encoding of Title-Case \"Hello\"\n",
    "sequence_1 = \"Hello\"\n",
    "encoding_1 = params.tokenizer.encode_plus(\n",
    "                        sequence_1,\n",
    "                        add_special_tokens = True,\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt'\n",
    "                   )\n",
    "\n",
    "\n",
    "token_id_1 = encoding_1['input_ids']\n",
    "attention_masks_1 = encoding_1['attention_mask']\n",
    "\n",
    "# View Encoding of Lower-Case \"hello\"\n",
    "sequence_2 = \"hello\"\n",
    "encoding_2 = params.tokenizer.encode_plus(\n",
    "                        sequence_2,\n",
    "                        add_special_tokens = True,\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt'\n",
    "                   )\n",
    "\n",
    "\n",
    "token_id_2 = encoding_2['input_ids']\n",
    "attention_masks_2 = encoding_2['attention_mask']\n",
    "\n",
    "\n",
    "print_sentence_encoding(token_id_1, attention_masks_1)\n",
    "print_sentence_encoding(token_id_2, attention_masks_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ïí‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï§‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï§‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïï\n",
      "‚îÇ Tokens   ‚îÇ   Token IDs ‚îÇ   Attention Mask ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ <s>      ‚îÇ           0 ‚îÇ                1 ‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ Hello    ‚îÇ       31414 ‚îÇ                1 ‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ ƒ†world   ‚îÇ         232 ‚îÇ                1 ‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ ,        ‚îÇ           6 ‚îÇ                1 ‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ ƒ†        ‚îÇ        1437 ‚îÇ                1 ‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ ƒ†        ‚îÇ        1437 ‚îÇ                1 ‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ ƒ†the     ‚îÇ           5 ‚îÇ                1 ‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ ƒ†s       ‚îÇ         579 ‚îÇ                1 ‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ ld       ‚îÇ        4779 ‚îÇ                1 ‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ kj       ‚îÇ       36085 ‚îÇ                1 ‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ ƒ†u       ‚îÇ        1717 ‚îÇ                1 ‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ g        ‚îÇ         571 ‚îÇ                1 ‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ </s>     ‚îÇ           2 ‚îÇ                1 ‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ <pad>    ‚îÇ           1 ‚îÇ                0 ‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ <pad>    ‚îÇ           1 ‚îÇ                0 ‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ <pad>    ‚îÇ           1 ‚îÇ                0 ‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ <pad>    ‚îÇ           1 ‚îÇ                0 ‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ <pad>    ‚îÇ           1 ‚îÇ                0 ‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ <pad>    ‚îÇ           1 ‚îÇ                0 ‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ <pad>    ‚îÇ           1 ‚îÇ                0 ‚îÇ\n",
      "‚ïò‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïß‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïß‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïõ\n"
     ]
    }
   ],
   "source": [
    "sequence = \"Hello world,   the sldkj ug\"\n",
    "test = params.tokenizer.encode_plus(\n",
    "                        sequence,\n",
    "                        add_special_tokens = True,\n",
    "                        max_length = 20,\n",
    "                        padding='max_length',\n",
    "                        truncation=True,\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt'\n",
    "                   )\n",
    "\n",
    "\n",
    "token_id = test['input_ids']\n",
    "attention_masks =test['attention_mask']\n",
    "\n",
    "print_sentence_encoding(token_id, attention_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing Training Data\n",
    "\n",
    "Our preprocessing function, located in `utils.py`,  will tokenize and encode our strings according to RoBERTa's pre-defined tokenization and encoding dictionary. The returned result is a dictionary that includes:\n",
    "\n",
    "<ul>\n",
    "\n",
    "<b>token/input ids</b> : A list of integers that represent our tokenized and encoded string<br />\n",
    "<b>attention masks</b> : A list of 1's and 0's mapped to each token id. 1 represents an id to which the model should apply attention and 0 represents an id to which the model may not apply attention (padding tokens, for example).\n",
    "\n",
    "</ul>\n",
    "\n",
    "The dictionary is parsed out to lists which we can then convert to tensors and construct training/validation datasets that are acceptable for our dataloaders later in this notebook. We also note the following:\n",
    "\n",
    "- Our preprocessing function includes Huggingface's `tokenizer.encode_plus` method, for simplicity.\n",
    "- The following can be done more succinctly, for example, with our data stored as a `datasets.Dataset` object which enables features such as mapping functions to all splits of our data at once. However, the below loop is a bit more illustrative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_token_id = []\n",
    "train_attention_masks = []\n",
    "\n",
    "# encode training text\n",
    "for sample in train_df.text.values:\n",
    "  encoding_dict = preprocessing(sample, params.tokenizer)\n",
    "  # parse encoding dict to lists\n",
    "  train_token_id.append(encoding_dict['input_ids']) \n",
    "  train_attention_masks.append(encoding_dict['attention_mask'])\n",
    "\n",
    "# create tensors for our token ids, attention masks, and labels\n",
    "train_token_id = torch.cat(train_token_id, dim = 0)\n",
    "train_attention_masks = torch.cat(train_attention_masks, dim = 0)\n",
    "train_labels = torch.tensor(train_df.label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode validation text \n",
    "validate_token_id = []\n",
    "validate_attention_masks = []\n",
    "\n",
    "# encode training text\n",
    "for sample in validate_df.text.values:\n",
    "  encoding_dict = preprocessing(sample, params.tokenizer)\n",
    "  # parse encoding dict to lists\n",
    "  validate_token_id.append(encoding_dict['input_ids']) \n",
    "  validate_attention_masks.append(encoding_dict['attention_mask'])\n",
    "\n",
    "# create tensors for our token ids, attention masks, and labels\n",
    "validate_token_id = torch.cat(validate_token_id, dim = 0)\n",
    "validate_attention_masks = torch.cat(validate_attention_masks, dim = 0)\n",
    "validate_labels = torch.tensor(validate_df.label.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we show that our inputs are either:\n",
    "1. Truncated to `max_length`, 512 tokens\n",
    "2. Padded to `max_length`, 512 tokens\n",
    "\n",
    "The result is that every input is of length 512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: [num_samples, max_input_size]\n",
      "torch.Size([36000, 512]) \n",
      "\n",
      "Length: 512\n",
      "tensor([    0,   863, 11278,     8, 11776,  7815,    32,   233,     9,    41,\n",
      "         8180, 15893,     9,   559,   822,  6644,     4,    20,  9004,  1956,\n",
      "          197,  5478,    14,    51,   860,     7, 20489,     5,  3157,    77,\n",
      "            5,   168,     8,   433,  7313,  7018,  8807,     8,  1719,    62,\n",
      "            5,  1061,  3817,     5,  1281,   529,    11,  8432,  2457,  3321,\n",
      "            4,    85,    16,  3668, 17052,     7,   192,   141,   209,  7850,\n",
      "         1159,    33,    57,  9441,     8,   385, 31756,    30,     5,  9004,\n",
      "          249,     8,  3828,     7,  1789,    11,   559,  7341,    13,  3981,\n",
      "         2788,  3731,     8,    25, 13543,    13,   643,  2163,     4,    20,\n",
      "          129,  9327,   631,    59,    42,  1569,    16,    14,    24,    40,\n",
      "           45,  1338,     5,  4007, 15444,    11,  6171,    25,    24,    40,\n",
      "          129,    28,  2343,    24, 11327,     8,    45,    28,   703,    15,\n",
      "          569,    50, 10843,    15,  2384, 49069,  3809,  1589, 49007,  3809,\n",
      "        48709,   133,   559,   822,    16,   505,    25,    24,    64,   836,\n",
      "           92, 17403,     8,  8339,    88,  2632,   743,     8,    34,    10,\n",
      "          774,     7,   310,    25,    41, 23356,     9,     5, 15444,     4,\n",
      "            2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1])\n",
      "<s>Jarl and Moodysson are part of an dying breed of political film makers. The Swedish population should appreciate that they try to uncover the truth when the government and media actively distorts and cover up the events surrounding the EU meeting in Gothenburg. It is absolutely heartbreaking to see how these innocent kids have been abused and drugged by the Swedish police and convicted to prison in political trials for sending text messages and as revenge for others actions. The only unfortunate thing about this movie is that it will not reach the broad masses in Sweden as it will only be shown it theaters and not be released on video or aired on television.<br /><br />The political film is important as it can bring new perspectives and insight into complex issues and has a role to play as an educator of the masses.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "# view token_id shape\n",
    "print(\"Size: [num_samples, max_input_size]\")\n",
    "print(train_token_id.size(), \"\\n\")\n",
    "\n",
    "# view and decode a single sequence\n",
    "print(\"Length:\", len(train_token_id[(13)]))\n",
    "print(train_token_id[13])\n",
    "print(params.tokenizer.decode(train_token_id[13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: [token_length]\n",
      "Token @ index 10:  torch.Size([512])\n",
      "Token @ index 30,001:  torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "# view token_id shape\n",
    "print(\"Size: [token_length]\")\n",
    "print(\"Token @ index 10: \", train_token_id[10].size())\n",
    "print(\"Token @ index 30,001: \", train_token_id[30001].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Input Datasets\n",
    "Here we construct the dataset objects containing all of the necessary data for fine-tuning. `torch.utils.data.Dataset` allows us to use pre-loaded datasets as well as our own data. Datasets store the samples and their corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and validation sets\n",
    "train_set = TensorDataset(train_token_id, \n",
    "                          train_attention_masks, \n",
    "                          train_labels)\n",
    "\n",
    "val_set = TensorDataset(validate_token_id, \n",
    "                        validate_attention_masks, \n",
    "                        validate_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then give our dataset objects to dataloaders. `torch.utils.data.DataLoader` wraps an iterable around the Dataset to enable easy access to the samples.\n",
    "\n",
    "<a href=\"https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\"> View the Pytorch Datasets & Dataloaders Documentation</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare DataLoaders\n",
    "train_dataloader = DataLoader(\n",
    "            train_set,\n",
    "            batch_size = params.batch_size,\n",
    "        )\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            val_set,\n",
    "            batch_size = params.batch_size,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[    0, 29287, 34717,  ...,     1,     1,     1],\n",
       "         [    0,  8346,     6,  ...,     1,     1,     1],\n",
       "         [    0,   100,    78,  ...,     1,     1,     1],\n",
       "         ...,\n",
       "         [    0,   863, 11278,  ...,     1,     1,     1],\n",
       "         [    0,   100,  1017,  ...,     1,     1,     1],\n",
       "         [    0, 35634,  1268,  ...,     1,     1,     1]]),\n",
       " tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " tensor([0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view an example from the dataloader\n",
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Before training, we must instantiate our core model. Here, we download transformers.RobertaForSequenceClassification from HuggingFace, which is a RoBERTa model with a linear layer for sentence classification (or regression) on top of the pooled output.\n",
    "\n",
    "The model we load here may come from any source. Pytorch, for example, also makes pre-trained models available for fine-tuning. Depending on the source, we may or may not have to manually attach our own classification head. We use HuggingFace in this example as it is particularly simple to load a models with new classification heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==============================================================================================================\n",
       "Layer (type:depth-idx)                                       Output Shape              Param #\n",
       "==============================================================================================================\n",
       "RobertaForSequenceClassification                             [1, 2]                    --\n",
       "‚îú‚îÄRobertaModel: 1-1                                          [1, 512, 768]             --\n",
       "‚îÇ    ‚îî‚îÄRobertaEmbeddings: 2-1                                [1, 512, 768]             --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄEmbedding: 3-1                                   [1, 512, 768]             38,603,520\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄEmbedding: 3-2                                   [1, 512, 768]             768\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄEmbedding: 3-3                                   [1, 512, 768]             394,752\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-4                                   [1, 512, 768]             1,536\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-5                                     [1, 512, 768]             --\n",
       "‚îÇ    ‚îî‚îÄRobertaEncoder: 2-2                                   [1, 512, 768]             --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄModuleList: 3-6                                  --                        85,054,464\n",
       "‚îú‚îÄRobertaClassificationHead: 1-2                             [1, 2]                    --\n",
       "‚îÇ    ‚îî‚îÄDropout: 2-3                                          [1, 768]                  --\n",
       "‚îÇ    ‚îî‚îÄLinear: 2-4                                           [1, 768]                  590,592\n",
       "‚îÇ    ‚îî‚îÄDropout: 2-5                                          [1, 768]                  --\n",
       "‚îÇ    ‚îî‚îÄLinear: 2-6                                           [1, 2]                    1,538\n",
       "==============================================================================================================\n",
       "Total params: 124,647,170\n",
       "Trainable params: 124,647,170\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 124.65\n",
       "==============================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 427.83\n",
       "Params size (MB): 498.59\n",
       "Estimated Total Size (MB): 926.42\n",
       "=============================================================================================================="
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the RobertaForSequenceClassification model\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base',\n",
    "                                                         num_labels = params.num_labels,\n",
    "                                                         output_attentions = False,\n",
    "                                                         output_hidden_states = False,\n",
    "                                                         )\n",
    "\n",
    "# view the model summary by passing dummy data of compatible shape\n",
    "from torchinfo import summary\n",
    "summary(model, input_size=(1, 512), dtypes=['torch.IntTensor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Dataset: data/inter_IMDB_sentiment/IMDB_train.csv\n",
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# set model to device\n",
    "model.to(params.device)\n",
    "print(f\"Trained Dataset: {train_dataset_path}\")\n",
    "print(f\"Device: {params.device}\")\n",
    "\n",
    "# define optimizer\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=params.learning_rate) #roberta\n",
    "\n",
    "# initialize trainer\n",
    "trainer = Trainer(model=model,\n",
    "                  device=params.device,\n",
    "                  tokenizer=params.tokenizer,\n",
    "                  train_dataloader=train_dataloader,\n",
    "                  validation_dataloader=validation_dataloader,\n",
    "                  epochs=params.epochs,\n",
    "                  optimizer=optimizer,\n",
    "                  val_loss_fn=params.val_loss_fn,\n",
    "                  num_labels=params.num_labels,\n",
    "                  output_dir=params.output_dir,\n",
    "                  save_freq=params.save_freq,\n",
    "                  checkpoint_freq=params.checkpoint_freq,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model to our training data - fine-tuning\n",
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: left;\"><img src=\"presentation_resources/imdb_512_training.png\"  align=\"left\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Once a model is fine-tuned, we can load the model, its tokenizer, pre-process new input, and generate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports for Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "from transformers import TextClassificationPipeline\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infer w/Pytorch\n",
    "\n",
    "Our first example quickly shows how to infer with Pytorch, which involves instantiating the pre-trained model and applying our fine-tuned model's state. We can then put the model into eval mode and run inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get State Dict Location & Tokenizer Path\n",
    "pt_check_path = 'models/imdb_512/E02_A0.96_F0.96/checkpoint.pt'\n",
    "tokenizer_path = 'models/imdb_512/E02_A0.96_F0.96'\n",
    "\n",
    "# Instantiate core model\n",
    "# This can be done with Huggingface, from local, from Pytorch, etc\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base',\n",
    "                                                         num_labels = params.num_labels,\n",
    "                                                         output_attentions = False,\n",
    "                                                         output_hidden_states = False,\n",
    "                                                         )\n",
    "\n",
    "# Get Tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(tokenizer_path, local_files_only=True)\n",
    "\n",
    "# load and apply fine-tuned model state to core model\n",
    "checkpoint = torch.load(pt_check_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# set to eval mode\n",
    "model.eval()\n",
    "\n",
    "# tokenize inputs\n",
    "inputs = tokenizer([\"I hate this movie!\", \"I love this movie!\"], return_tensors=\"pt\")\n",
    "\n",
    "# infer\n",
    "output = model(inputs['input_ids'], inputs['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>I hate this movie!</s> 0\n",
      "<s>I love this movie!</s> 1\n"
     ]
    }
   ],
   "source": [
    "# get logits from output\n",
    "logits = output.logits\n",
    "\n",
    "# loop through logits\n",
    "for i, v in enumerate(logits):\n",
    "    # get index of largest value\n",
    "    predicted_class_id = v.argmax().item()\n",
    "    # get & decode input, match with predicted_class_id\n",
    "    print(params.tokenizer.decode((inputs['input_ids'][i])), predicted_class_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infer with HugginFace Pipeline\n",
    "\n",
    "HuggingFace's pipeline method allows us to streamline the inference process further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'models/imdb_512/E02_A0.96_F0.96'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(PATH, local_files_only=True)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(PATH, local_files_only=True)\n",
    "\n",
    "# define pipeline\n",
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, top_k=2, max_length=512, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'LABEL_0', 'score': 0.9976567029953003},\n",
       "  {'label': 'LABEL_1', 'score': 0.002343281637877226}]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"i hate this movie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Whole Test Set & Evaluate\n",
    "We will use the HuggingFace pipeline with a loop in order to generate predictions for our entire test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Test Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>With No Dead Heroes you get stupid lines like ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I thought maybe... maybe this could be good. A...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>An elite American military team which of cours...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ridiculous horror film about a wealthy man (Jo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Well, if you are one of those Katana's film-nu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  With No Dead Heroes you get stupid lines like ...      0\n",
       "1  I thought maybe... maybe this could be good. A...      0\n",
       "2  An elite American military team which of cours...      0\n",
       "3  Ridiculous horror film about a wealthy man (Jo...      0\n",
       "4  Well, if you are one of those Katana's film-nu...      1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Test Data\n",
    "test_df = pd.read_csv('data/inter_IMDB_sentiment/IMDB_test.csv')\n",
    "\n",
    "# test_df = test_df.sample(n=500, random_state=1)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sequences\n",
    "test_input = test_df['text'].to_list()\n",
    "\n",
    "test_output = []\n",
    "\n",
    "# pipe sequences to tokenizer -> model\n",
    "with tqdm(test_input, unit=\"test\") as prog:\n",
    "    for step, test in enumerate(prog):\n",
    "        prog.set_description(f\"Test {step+1}\")\n",
    "        # append results to test_output list\n",
    "        test_output.append(pipe(test)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Output Slice:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'label': 'LABEL_0', 'score': 0.998106837272644},\n",
       "  {'label': 'LABEL_1', 'score': 0.001893149223178625}],\n",
       " [{'label': 'LABEL_0', 'score': 0.9981493949890137},\n",
       "  {'label': 'LABEL_1', 'score': 0.0018506310880184174}],\n",
       " [{'label': 'LABEL_0', 'score': 0.9985480904579163},\n",
       "  {'label': 'LABEL_1', 'score': 0.00145185727160424}],\n",
       " [{'label': 'LABEL_0', 'score': 0.9984475374221802},\n",
       "  {'label': 'LABEL_1', 'score': 0.0015524596674367785}],\n",
       " [{'label': 'LABEL_1', 'score': 0.5626480579376221},\n",
       "  {'label': 'LABEL_0', 'score': 0.43735191226005554}]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Test Output Slice:\")\n",
    "test_output[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "['LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_1', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0']\n"
     ]
    }
   ],
   "source": [
    "# parse target predictions to new list\n",
    "predictions = []\n",
    "\n",
    "for i in test_output:\n",
    "    predictions.append(i[0]['label'])\n",
    "    \n",
    "print(len(predictions))\n",
    "print(predictions[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# remove \"LABEL_\" and cast as int\n",
    "for i, v in enumerate(predictions):\n",
    "    predictions[i] = int(v.replace(\"LABEL_\",\"\"))\n",
    "\n",
    "print(predictions[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.955\n",
      "F1:  0.9545270816491511\n"
     ]
    }
   ],
   "source": [
    "# get accuracy and F1\n",
    "acc = accuracy_score(test_df['label'], predictions)\n",
    "f1 = f1_score(test_df['label'], predictions)\n",
    "\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"F1: \", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚è© How Do We Optimize?\n",
    "The training process presented in this notebook is quite slow. How can we optimize to speed the process up without much or any impact to accuracy/F1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center;\"><img src=\"presentation_resources/giphy.gif\" width=\"750\" align=\"center\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We have two simple options, both of which involve methods that shorten the input lengths:\n",
    "1. Decrease `max_length`\n",
    "2. Dynamic Padding\n",
    "\n",
    "<ul>\n",
    "\n",
    "<b>Decrease `max_length`</b> : The maximum number of tokens per sequence that RoBERTa can accept is 512. However, we may consider that the information necessary to classify our sequences accurately is available within far fewer than 512 tokens. We can truncate our input sequences to 256 tokens, for example, which would halve the number of calculations performed by comparison to inputs of 512 tokens.\n",
    "\n",
    "By exploring the training data a bit more thoroughly, we can support this argument further by noticing that the majority of our inputs will be of sequence length 256 or shorter.\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize training sequences and get lengths\n",
    "unpadded_token_id = []\n",
    "\n",
    "for sample in train_df.text.values:\n",
    "  encoding_dict = preprocessing_trunkless(sample, params.tokenizer)\n",
    "  unpadded_token_id.append(encoding_dict['input_ids']) \n",
    "\n",
    "lengths = []\n",
    "count_256 = 0\n",
    "count_512 = 0\n",
    "for i in unpadded_token_id:\n",
    "    i_length = len(i)\n",
    "\n",
    "    if i_length <= 256:\n",
    "      count_256 += 1\n",
    "\n",
    "    if i_length <= 512:\n",
    "      count_512 += 1\n",
    "\n",
    "    lengths.append(i_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset Size: 36000\n",
      "Num Sequences with Token Lengths <= 256: 21029\n",
      "Num Sequences with Token Lengths <= 512: 31211\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize lengths of training sequences\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# setup the plot\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "plt.hist(lengths, bins=50)\n",
    "# now, define the ticks (i.e. locations where the labels will be plotted)\n",
    "xticks = [256, 512, 750, 1000, 1500, 2000]\n",
    "\n",
    "# also define the labels we'll use (note this MUST have the same size as `xticks`!)\n",
    "xtick_labels = [256, 512, 750, 1000, 1500, 2000]\n",
    "\n",
    "# add the ticks and labels to the plot\n",
    "ax.set_xticks(xticks)\n",
    "ax.set_xticklabels(xtick_labels)\n",
    "\n",
    "print(f\"Total dataset Size: {len(unpadded_token_id)}\")\n",
    "print(f\"Num Sequences with Token Lengths <= 256: {count_256}\")\n",
    "print(f\"Num Sequences with Token Lengths <= 512: {count_512}\")\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "\n",
    "<b>Dynamic Padding</b> : We may also employ a process called \"Dynamic Padding\". Because models accept tensors as inputs, and nested tensors on the same dimension must be of the same length, this notebook pads ALL inputs to the `max_length` value so that our inputs may be stored in the input tensor. Alternatively, we may leverage a collation function and our dataloaders to pad sequences and create tensors after each batch is created, but before each batch is handed to the model. This allows us to pad each batch to the length of the longest tensor in the given batch, decreasing input sizes overall.\n",
    "\n",
    "</ul>\n",
    "\n",
    "View our last notebook, `03_IMDB_dyna.ipynb`, to see how we can implement shorter `max_length` and dynamic padding on this dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('workshop_present_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "67962405cad10eeeccd3e4011c192b84211e6b516afcf058c81650d23e67a1ed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
